---
title: "ML_Assignment6"
output: html_document
date: "2023-02-28"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Load needed packages

```{r}

library(tidyverse)
library(rpart)
library(caret)
library(rpart.plot)
library(pROC)
library(NHANES)
library(e1071)

```

### Load and check data

```{r}
# Load the NHANES dataset
data(NHANES)

# Subset the NHANES dataset to only include the 11 variables of interest
nhanes_data <- NHANES[, c("Age", "Race1", "Education", "HHIncome", "Weight", "Height", "Pulse", "Diabetes", "BMI", "PhysActive", "Smoke100")]

# View the first few rows of the subsetted data frame
head(nhanes_data)

nhanes_data <- na.omit(nhanes_data)
```

### Partition data into training and testing

```{r}
set.seed(123)

train.indices<-createDataPartition(y=nhanes_data$Diabetes,p=0.7,list=FALSE)
training<-nhanes_data[train.indices,]
testing<-nhanes_data[-train.indices,]
```

```{r}
set.seed(123)
# Check the frequency of the outcome variable in the training dataset
table(training$Diabetes)

# Check the frequency of the outcome variable in the testing dataset
table(testing$Diabetes)
```


### Model 1: Classification Trees

```{r classtree}
set.seed(123)

#Creating 10-fold cross-validation
train_control_class<-trainControl(method="cv", number=10, sampling="down")

#Create sequence of cp parameters to try 
grid.2 <- expand.grid(cp = seq(0.001, 0.01, by = 0.001))

#Train model
tree_model <-train(Diabetes ~ ., data = training, method="rpart",trControl=train_control_class, tuneGrid=grid.2)

tree_model$bestTune

tree_model

rpart.plot(tree_model$finalModel)

#Variable importance on the final model within training data
varImp(tree_model)

#Accuracy metric and confusion matrix from training
confusionMatrix(tree_model)
```

### Model 2: Logistic regression 

```{r}
set.seed(123)

# Create 10-fold cross-validation
train_control_logistic <- trainControl(method = "cv", number = 10, sampling="down")

# Train logistic regression model
logistic_caret <- train(Diabetes ~ ., data = training, method = "glm", family = "binomial", trControl = train_control_logistic, preProcess = c("center", "scale"))

# Print the results
logistic_caret

# Incorporate different values for lambda
logistic_caret_2 <- train(Diabetes ~ ., data = nhanes_data, method = "glmnet", family = "binomial", trControl = train_control_logistic, preProcess = c("center", "scale"), tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 1, length = 30)))


plot(logistic_caret_2)

# Accuracy metric and confusion matrix from training
confusionMatrix(logistic_caret_2)


```

### Model 3: Support Vector Classifiers

```{r}

modelLookup("svmLinear")

set.seed(123)

#Set 10-fold cross-validation
train_control_svm<-trainControl(method="cv", number=10, sampling="down", classProbs = T)

#Train model
svm_caret<-train(Diabetes ~ ., data=training, method="svmLinear", trControl=train_control_svm, preProcess=c("center", "scale"))

svm_caret

# Train model
svm_model <- train(Diabetes ~ ., data = training, method = "svmLinear", trControl = train_control_svm,preProcess=c("center", "scale"), tuneGrid=expand.grid(C=seq(0.001,2, length=20)))

#Visualize accuracy versus values of C
plot(svm_model)

#Obtain metrics of accuracy from training
confusionMatrix(svm_model)

```


### Select a "optimal" model and calculate final evaluation metrics in the test set

After comparing the average accuracy among three different models, Support Vector Classifiers has the highest average accuracy with a better performance. 
```{r}
set.seed(123)
#Make predictions in testset
svm_model_pred<-predict(svm_model, testing)

#Get evaluation metrics from test set
confusionMatrix(svm_model_pred, testing$Diabetes, positive="Yes")

#Create ROC Curve for Analysis
pred_prob<-predict(svm_model, testing, type="prob")

```

#### *List and describe at least two limitations/considerations of the model generated by this analysis. Limitations can be analytical or they can be considerations that need to be made regarding how the model would be applied in practice.*

Comments:

1. Generalizability: These models were developed using a specific dataset (NHANES) and may not necessarily generalize to other populations or settings. It's important to validate the model on external datasets to ensure that it can perform well in different contexts.

Check the frequency of the outcome variable in the training dataset

`Diabetes`  
No (3988)  Yes (462) 

Check the frequency of the outcome variable in the testing dataset

`Diabetes`   
No (1709) Yes (197)

2. Imbalanced classes: The Diabetes variable in the cleaning dataset is imbalanced, with only 10.4% of observations being positive cases. This can lead to biased model performance, where the model is more accurate at predicting the majority class (no diabetes) and less accurate at predicting the minority class (diabetes). This is especially important to consider in practice, where the costs of false negatives (not detecting diabetes in a patient who has it) can be high. It may be necessary to balance the classes or use different performance metrics that take into account the imbalanced nature of the data.
